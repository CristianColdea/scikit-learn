{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,) (11,)\n",
      "(11, 1)\n",
      "[0.         0.15999738]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 15\n",
    "x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "#print(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train = X_train.reshape((-1,1))\n",
    "print(X_train.shape)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "linreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "j = 4\n",
    "j += 1\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(shape=(4,1))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "a.flatten()\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "a.reshape((4,))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.08877265],\n",
       "       [ 3.23065446],\n",
       "       [ 1.62431903],\n",
       "       [ 9.31004929],\n",
       "       [ 7.17166586],\n",
       "       [ 4.96972856],\n",
       "       [ 8.14799756],\n",
       "       [ 2.59103578],\n",
       "       [ 0.35281047],\n",
       "       [ 3.375973  ],\n",
       "       [ 8.72363612]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 3, 6, 2, 4, 8, 7])\n",
    "inds = a.argsort()[-3:][::-1]\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.96972856],\n",
       "       [8.14799756],\n",
       "       [1.62431903]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.logspace(-2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-02, 1.e+00, 1.e+02])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.logspace(-4, 1, 6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 ** -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'A':[1, 1, 6, 3, 2, 2], 'B':[3, 5, 1, 4, 5, 4]}\n",
    "df = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  3\n",
      "1  1  5\n",
      "2  6  1\n",
      "3  3  4\n",
      "4  2  5\n",
      "5  2  4\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1.649384\n",
      "B   -1.269815\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = stats.boxcox(df['A'])\n",
    "x2 = stats.boxcox(df['B'])\n",
    "#print(x1[0])\n",
    "df['A'] = x1[0]\n",
    "df['B'] = x2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  0.000000  3.387892\n",
      "1  0.000000  9.204492\n",
      "2  1.299753  0.000000\n",
      "3  0.898230  6.014812\n",
      "4  0.609418  9.204492\n",
      "5  0.609418  6.014812\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    0.127218\n",
      "B   -0.675747\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': [1, 1, 6, 3, 2, 2], 'B': [3, 5, 1, 4, 5, 4]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  3\n",
      "1  1  5\n",
      "2  6  1\n",
      "3  3  4\n",
      "4  2  5\n",
      "5  2  4\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame(dict)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1.649384\n",
      "B   -1.269815\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df2.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  0.000000  3.387892\n",
      "1  0.000000  9.204492\n",
      "2  1.299753  0.000000\n",
      "3  0.898230  6.014812\n",
      "4  0.609418  9.204492\n",
      "5  0.609418  6.014812\n",
      "A    0.127218\n",
      "B   -0.675747\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df2['A'] = stats.boxcox(df2['A'])[0]\n",
    "df2['B'] = stats.boxcox(df2['B'])[0]\n",
    "print(df2)\n",
    "print(df2.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B\n",
      "0   1  -2\n",
      "1  -2  -3\n",
      "2  19   4\n",
      "3   1  20\n",
      "4  -3   2\n",
      "5   2   0\n",
      "A    2.130047\n",
      "B    1.970745\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dict1 = {'A':[1, -2, 19, 1, -3, 2], 'B':[-2, -3, 4, 20, 2, 0]}\n",
    "df3 = pd.DataFrame(dict1)\n",
    "print(df3)\n",
    "print(df3.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  1.609438  0.693147\n",
      "1  0.693147  0.000000\n",
      "2  3.135494  2.079442\n",
      "3  1.609438  3.178054\n",
      "4  0.000000  1.791759\n",
      "5  1.791759  1.386294\n",
      "A    0.237450\n",
      "B    0.142393\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in df3.columns:\n",
    "    df3[col] = np.log(df3[col] + 1 - np.min(df3[col]))\n",
    "    \n",
    "print(df3)\n",
    "print(df3.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B\n",
      "0   1  -2\n",
      "1  -2  -3\n",
      "2  19   4\n",
      "3   1  20\n",
      "4  -3   2\n",
      "5   2   0\n",
      "A    2.130047\n",
      "B    1.970745\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.DataFrame(dict1)\n",
    "print(df4)\n",
    "print(df4.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  1.509660  0.680802\n",
      "1  0.674187  0.000000\n",
      "2  2.771503  1.970946\n",
      "3  1.509660  2.929308\n",
      "4  0.000000  1.710811\n",
      "5  1.668685  1.337500\n",
      "A    0.024581\n",
      "B    0.021305\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in df4.columns:\n",
    "    df4[col] = stats.boxcox(df4[col] + 1 - np.min(df4[col]))[0]\n",
    "\n",
    "print(df4)\n",
    "print(df4.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def blight_model():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    #reading train dataset\n",
    "    df_train = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_test = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_addresses = pd.read_csv(\"addresses.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_latlons = pd.read_csv(\"latlons.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    \n",
    "    #getting the common features into a list\n",
    "    commons_test = list(set(df_train.columns).intersection(df_test.columns))\n",
    "    commons_train = commons_test + ['compliance']\n",
    "\n",
    "    df_train = df_train[commons_train]\n",
    "    df_test = df_test[commons_test]\n",
    "        \n",
    "    #join 'addresses' and 'latlons' on 'address' column\n",
    "    df_joined = df_addresses.set_index('address').join(df_latlons.set_index('address'),\n",
    "                                                       how='outer')\n",
    "    \n",
    "    #join 'joined' on 'train' and 'test', on 'ticket_id' as index\n",
    "    df_train = df_train.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    df_test = df_test.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    \n",
    "    #cleaning datasets  \n",
    "    df_train.dropna(subset=['compliance'], how='any', inplace=True)\n",
    "    \n",
    "    df_train.fillna(value=0, inplace=True)\n",
    "    df_test.fillna(value=0, inplace=True)\n",
    "    \n",
    "    df_train[['compliance']] = df_train[['compliance']].astype('float64')\n",
    "\n",
    "    \n",
    "    \n",
    "    #transform date columns delta into float, 'hearing_date' and 'payment_date'\n",
    "\n",
    "    df_train['hearing_date'] = pd.to_datetime(df_train['hearing_date'])\n",
    "    df_train['ticket_issued_date'] = pd.to_datetime(df_train['ticket_issued_date'])\n",
    "    df_train['days'] = df_train['hearing_date'] - df_train['ticket_issued_date']\n",
    "    df_train.days = (df_train.days.dt.days).astype('float64')\n",
    "    df_train.drop(['ticket_issued_date', 'hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    #for test data too\n",
    "    df_test['hearing_date'] = pd.to_datetime(df_test['hearing_date'])\n",
    "    df_test['ticket_issued_date'] = pd.to_datetime(df_test['ticket_issued_date'])\n",
    "    df_test['days'] = df_test['hearing_date'] - df_test['ticket_issued_date']\n",
    "    df_test.days = (df_test.days.dt.days).astype('float64')\n",
    "    df_test.drop(['ticket_issued_date','hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    to_drop = ['state',\n",
    "              'violation_street_number',\n",
    "              'zip_code',\n",
    "              'inspector_name',\n",
    "              'violator_name',\n",
    "              'mailing_address_str_name',\n",
    "              'mailing_address_str_number',\n",
    "              'violation_zip_code',\n",
    "              'admin_fee',\n",
    "              'state_fee',\n",
    "              'clean_up_cost',\n",
    "              'non_us_str_code',\n",
    "              'country']\n",
    "    \n",
    "    df_train.drop(to_drop, axis=1, inplace=True)\n",
    "    df_test.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    df_test['grafitti_status'] = df_test['grafitti_status'].replace(['GRAFFITI TICKET'], 0)\n",
    "    df_test['city'] = df_test['city'].astype('str')\n",
    "    \n",
    "    objs = []\n",
    "    for col in df_test.columns:\n",
    "        if df_test[col].dtype == 'object':\n",
    "            objs.append(col)\n",
    "    \n",
    "    #convert categorical variable into integers\n",
    "    \n",
    "    #['violation_code', 'grafitti_status', 'city', \n",
    "    #'violation_description', 'agency_name', 'violation_street_name', 'disposition']\n",
    "    \n",
    "    LE = preprocessing.LabelEncoder()\n",
    "    \n",
    "    for obj in objs:\n",
    "        df_train[obj] = LE.fit_transform(df_train[obj])\n",
    "        df_test[obj] = LE.fit_transform(df_test[obj])\n",
    "    \n",
    "    #print('Train skew:', '\\n', df_train.skew(), '\\n')\n",
    "    #print('Train columns:', '\\n', df_train.columns, '\\n')\n",
    "    #print('Test skew:', '\\n', df_test.skew())\n",
    "    #print('Test columns:', '\\n', df_test.columns, '\\n')\n",
    "    \n",
    "    \n",
    "    for col in df_train.columns:\n",
    "        df_train[col] = df_train[col].astype('float64')\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = df_test[col].astype('float64')\n",
    "        \n",
    "    train_cols = ['violation_code', 'grafitti_status', 'fine_amount', 'city',\n",
    "       'violation_description', 'judgment_amount', 'discount_amount',\n",
    "       'agency_name', 'violation_street_name', 'disposition', 'late_fee',\n",
    "       'lat', 'lon', 'days','compliance']\n",
    "    df_train = df_train[train_cols]\n",
    "    #print(df_train.columns)\n",
    "    \n",
    "    from scipy import stats\n",
    "    for col in df_train.columns[0:len(df_train.columns)-1]:\n",
    "        df_train[col] = stats.boxcox(df_train[col] + 1 - np.min(df_train[col]))[0]\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = stats.boxcox(df_test[col] + 1 - np.min(df_test[col]))[0]\n",
    "        \n",
    "    #print('De-skewed train: ', df_train.skew())\n",
    "    #print('De-skewed test: ', df_test.skew())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #compute feature importance using ExtraTreeClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    \n",
    "    forest = ExtraTreesClassifier(n_estimators=50)\n",
    "    feats_cols = df_train.columns[0:len(df_train.columns)-1]\n",
    "    X_selection = df_train[feats_cols]\n",
    "    y_selection = df_train['compliance']\n",
    "    forest.fit(X_selection, y_selection)\n",
    "    importances = forest.feature_importances_\n",
    "    new_cols = df_train.columns[np.argsort(importances)[::-1]]   #the order of most important features\n",
    "    #print(new_cols)\n",
    "    \n",
    "    for i in range(len(new_cols) - 1):\n",
    "        feats = new_cols[:i+1]\n",
    "        X = df_train[feats]\n",
    "        #print(X.columns)\n",
    "        #X = df.loc[:, df.columns != 'compliance']\n",
    "        #X.set_index('ticket_id', inplace=True)\n",
    "        y = df_train['compliance']\n",
    "        #print(X.head())\n",
    "        cols_test = df_test.columns\n",
    "        X_testF = df_test[cols_test]\n",
    "        #X_testF.set_index('ticket_id', inplace=True)\n",
    "        #print(X_testF.head())\n",
    "        \n",
    "        #splitting X, y\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "        #normalizing\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaler = scaler.fit_transform(X_train)\n",
    "        X_test_scaler = scaler.fit_transform(X_test)\n",
    "        X_test_scalerF = scaler.fit_transform(X_testF)\n",
    "\n",
    "        #print(X_train_scaler.shape)\n",
    "        #print(X_test_scaler.shape)\n",
    "        #print(X_test_scalerF.shape)\n",
    "\n",
    "        #set up the GradientBoostingClassifier\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "            max_depth=6, random_state=0).fit(X_train_scaler, y_train)\n",
    "\n",
    "\n",
    "        #clf = LogisticRegression(C=2.0, \n",
    "                                 #class_weight='balanced',\n",
    "                                 #random_state=0, \n",
    "                                 #solver='lbfgs', \n",
    "                                 #multi_class='ovr'\n",
    "                                #)\n",
    "\n",
    "        #clf = GaussianNB()\n",
    "    \n",
    "\n",
    "        \"\"\"\n",
    "        #GridSearchCV optimization\n",
    "        #from sklearn.grid_search import GridSearchCV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        for i in range(0,5):\n",
    "            randomly_sampled = df.ix[np.random.choice(df.index, 1000)]\n",
    "\n",
    "\n",
    "        gb_grid_params = {'max_depth': [4, 6, 8, 10, 12, 14, 16],\n",
    "                          'min_samples_split': [200, 400, 500, 700, 1000],\n",
    "                  }\n",
    "\n",
    "        gbc_grid = GridSearchCV(gbc,\n",
    "                                gb_grid_params,\n",
    "                                cv=5,\n",
    "                                scoring='roc_auc', \n",
    "                                n_jobs=-1)\n",
    "\n",
    "\n",
    "        #gbc_grid.fit(X_train_scaler, y_train)\n",
    "        #print('Grid best AUC', gbc_grid.best_params_)\n",
    "        #print('Best AUC is', gbc_grid.best_score)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        y_score_clf = clf.fit(X_train_scaler, y_train).predict(X_test_scaler)\n",
    "        fpr_clf, tpr_clf, _ = roc_curve(y_test, y_score_clf)\n",
    "        roc_auc_clf = auc(fpr_clf, tpr_clf)\n",
    "        print('AUC score is:', roc_auc_clf)\n",
    "        \n",
    "    #toreturn = clf.predict_proba(X_test_scalerF)\n",
    "    #print(toreturn[:10,1])\n",
    "    #answer = pd.Series(data=toreturn[:,1], index=df_test.index, dtype='float32', name=None, copy=False, fastpath=False)\n",
    "    #answer.index = answer.index.astype('int32')\n",
    "    #print(answer.head())\n",
    "    #print(df_test.index)\n",
    "    #print(toreturn)\n",
    "    \n",
    "    \n",
    "    return (clf.score(X_train_scaler, y_train), clf.score(X_test_scaler, y_test))\n",
    "    #return (gbc.score(X_train_scaler, y_train), gbc.score(X_test_scaler, y))\n",
    "    #return 'none'\n",
    "    #return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nimport numpy as np\\n\\ndef blight_model():\\n    \\n    # Your code here\\n    from sklearn.ensemble import RandomForestClassifier\\n    from sklearn.model_selection import train_test_split\\n    from sklearn.metrics import roc_auc_score\\n    #from sklearn.ensemble import GradientBoostingClassifier\\n    \\n    train_data = pd.read_csv('./train.csv',encoding = 'ISO-8859-1' )\\n    train_data = train_data[pd.notnull(train_data.compliance)]\\n    test_data = pd.read_csv('./test.csv',encoding='ISO-8859-1')\\n    test_data.set_index('ticket_id',inplace=True)\\n    \\n    COL = test_data.columns.tolist()\\n    target = ['compliance']\\n    \\n    features = [ 'fine_amount',\\n     'admin_fee',\\n     'state_fee',\\n     'late_fee',\\n     'discount_amount',\\n    # 'clean_up_cost',\\n     'judgment_amount',]\\n    #clf = GradientBoostingClassifier(n_estimators=400, learning_rate=1.0, max_depth=2, random_state=0)\\n    clf =RandomForestClassifier(n_estimators=100,max_depth=8,n_jobs=-1,random_state=0)\\n    X_train,X_valid,y_train,y_valid = train_test_split(train_data[features],train_data[target],test_size= 0.2,)\\n    clf.fit(train_data[features],train_data[target])\\n    #print(roc_auc_score(y_valid,clf.predict_proba(X_valid)[:,1],))\\n    y_pred = clf.predict_proba(test_data[features])[:,1]\\n    test_data['compliance'] = y_pred\\n    \\n    return test_data['compliance']# Your answer here\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def blight_model():\n",
    "    \n",
    "    # Your code here\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    train_data = pd.read_csv('./train.csv',encoding = 'ISO-8859-1' )\n",
    "    train_data = train_data[pd.notnull(train_data.compliance)]\n",
    "    test_data = pd.read_csv('./test.csv',encoding='ISO-8859-1')\n",
    "    test_data.set_index('ticket_id',inplace=True)\n",
    "    \n",
    "    COL = test_data.columns.tolist()\n",
    "    target = ['compliance']\n",
    "    \n",
    "    features = [ 'fine_amount',\n",
    "     'admin_fee',\n",
    "     'state_fee',\n",
    "     'late_fee',\n",
    "     'discount_amount',\n",
    "    # 'clean_up_cost',\n",
    "     'judgment_amount',]\n",
    "    \n",
    "    clf =RandomForestClassifier(n_estimators=100,max_depth=8,n_jobs=-1,random_state=0)\n",
    "    X_train,X_valid,y_train,y_valid = train_test_split(train_data[features],train_data[target],test_size= 0.2,)\n",
    "    clf.fit(train_data[features],train_data[target])\n",
    "    #print(roc_auc_score(y_valid,clf.predict_proba(X_valid)[:,1],))\n",
    "    y_pred = clf.predict_proba(test_data[features])[:,1]\n",
    "    test_data['compliance'] = y_pred\n",
    "    \n",
    "    return test_data['compliance']# Your answer here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  a  0\n",
      "1  b  1\n",
      "2  a  1\n",
      "3  c  1\n",
      "4  a  0\n",
      "5  b  0\n"
     ]
    }
   ],
   "source": [
    "dict1 = {'A':['a', 'b', 'a', 'c', 'a', 'b'], 'B':[0, 1, 1, 1, 0, 0]}\n",
    "df2 = pd.DataFrame(dict1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "a    0.333333\n",
      "b    0.500000\n",
      "c    1.000000\n",
      "Name: B, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "mean_a = df2.groupby('A').B.mean()\n",
    "print(mean_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(mean_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(mean_a.loc['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  a  0\n",
      "1  b  1\n",
      "2  a  1\n",
      "3  c  1\n",
      "4  a  0\n",
      "5  b  0\n"
     ]
    }
   ],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B         C\n",
      "0  a  0  0.333333\n",
      "1  b  1  0.500000\n",
      "2  a  1  0.333333\n",
      "3  c  1  1.000000\n",
      "4  a  0  0.333333\n",
      "5  b  0  0.500000\n"
     ]
    }
   ],
   "source": [
    "def apm(val):\n",
    "    return mean_a.loc[val]\n",
    "\n",
    "#df2['C'] = df2['A'].apply(apm)\n",
    "\n",
    "#arr = df2['A'].values\n",
    "#s = apm(lst).to_frame()\n",
    "#col = s.columns[0]\n",
    "#df2['C'] = apm(arr).values\n",
    "#s = apm(lst).reindex(df2.index.values)\n",
    "#df2.merge(s, left_index=True, right_index=True)\n",
    "#print(s.values)\n",
    "#print(col)\n",
    "#print(s[col])\n",
    "#df2['C'] = apm(lst).values\n",
    "#df2.drop('C', axis=1, inplace=True)\n",
    "df2['C'] = apm(df2['A'].values).values\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "\n",
    "def blight_model():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    #reading train dataset\n",
    "    df_train = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_test = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_addresses = pd.read_csv(\"addresses.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_latlons = pd.read_csv(\"latlons.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    \n",
    "    #getting the common features into a list\n",
    "    commons_test = list(set(df_train.columns).intersection(df_test.columns))   #test features\n",
    "    commons_train = commons_test + ['compliance']    #train features and labels\n",
    "\n",
    "    df_train = df_train[commons_train]\n",
    "    df_test = df_test[commons_test]\n",
    "        \n",
    "    #join 'addresses' and 'latlons' on 'address' column\n",
    "    df_joined = df_addresses.set_index('address').join(df_latlons.set_index('address'),\n",
    "                                                       how='outer')\n",
    "    \n",
    "    #join 'joined' on 'train' and 'test', on 'ticket_id' as index\n",
    "    df_train = df_train.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    df_test = df_test.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    \n",
    "    #cleaning datasets\n",
    "    \n",
    "    #print(df_train['compliance'].isnull().sum())\n",
    "    df_train['compliance'].fillna(\"NaN\", inplace=True)\n",
    "    df_train.dropna(subset=['compliance'], how='any', inplace=True)\n",
    "    #print(df_train['compliance'].isnull().sum())\n",
    "    \n",
    "    #print(df_train.shape, '\\t', df_test.shape)\n",
    "    \n",
    "    #check for full nulls column\n",
    "    #for col in df_test.columns:\n",
    "    #    print(col,'\\t', df_train[col].isnull().sum())\n",
    "    #    print(col,'\\t', df_test[col].isnull().sum())\n",
    "        \n",
    "    #drop columns\n",
    "    to_drop = ['admin_fee', 'state_fee', 'violation_zip_code',\n",
    "              'grafitti_status', 'non_us_str_code', 'inspector_name',\n",
    "              'violator_name', 'country']\n",
    "    \n",
    "    df_train.drop(to_drop, axis=1, inplace=True)\n",
    "    df_test.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "    #print(df_train.shape)\n",
    "    \n",
    "    #check for missing values and replace with NaNs\n",
    "    \n",
    "    #df_train['compliance'].dropna(inplace=True)\n",
    "    #print(df_train.shape, '\\t', df_test.shape)\n",
    "    \n",
    "    df_train.dropna(inplace=True)\n",
    "    df_train.fillna(value=0, inplace=True)\n",
    "    df_test.fillna(value=0, inplace=True)\n",
    "    \n",
    "    #for col in df_test.columns:\n",
    "    #    print(col,'\\t', df_train[col].isnull().sum())\n",
    "    #    print(col,'\\t', df_test[col].isnull().sum())\n",
    "    \n",
    "    #df_train.fillna(\"NaN\", inplace=True)\n",
    "    #df_test.fillna(\"NaN\", inplace=True)\n",
    "    \n",
    "    \n",
    "    #df_train.fillna(value=0, inplace=True)\n",
    "    #df_test.fillna(value=0, inplace=True)\n",
    "    \n",
    "    df_train[['compliance']] = df_train[['compliance']].astype('float64')\n",
    "\n",
    "    \n",
    "    \n",
    "    #transform date columns delta into float, 'hearing_date' and 'payment_date'\n",
    "\n",
    "    df_train['hearing_date'] = pd.to_datetime(df_train['hearing_date'])\n",
    "    df_train['ticket_issued_date'] = pd.to_datetime(df_train['ticket_issued_date'])\n",
    "    df_train['days'] = df_train['hearing_date'] - df_train['ticket_issued_date']\n",
    "    df_train.days = (df_train.days.dt.days).astype('float64')\n",
    "    df_train.drop(['ticket_issued_date', 'hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    #for test data too\n",
    "    \n",
    "    df_test['hearing_date'] = pd.to_datetime(df_test['hearing_date'])\n",
    "    df_test['ticket_issued_date'] = pd.to_datetime(df_test['ticket_issued_date'])\n",
    "    df_test['days'] = df_test['hearing_date'] - df_test['ticket_issued_date']\n",
    "    df_test.days = (df_test.days.dt.days).astype('float64')\n",
    "    df_test.drop(['ticket_issued_date','hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    #print(df_train.columns)\n",
    "    \n",
    "    df_test['city'] = df_test['city'].astype('str')   #cast any non-str to str in 'city' column\n",
    "    \n",
    "    #getting non numerical columns\n",
    "    objs = []\n",
    "    for col in df_test.columns:\n",
    "        if df_test[col].dtype == 'object':\n",
    "            objs.append(col)\n",
    "    \n",
    "    for obj in objs:\n",
    "        mean = df_train.groupby(obj).compliance.mean()\n",
    "        def apm(val):\n",
    "            return mean.loc[val]\n",
    "        df_train[obj] = apm(df_train[obj].values).values\n",
    "        df_test[obj] = apm(df_test[obj].values).values\n",
    "    \n",
    "    \n",
    "    imp = Imputer(missing_values='NaN', strategy=\"mean\")\n",
    "    \n",
    "    for col in df_train[0:len(df_train.columns)-1]:\n",
    "        df_train[col] = imp.fit_transform(df_train[col])\n",
    "        \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = imp.fit_transform(df_test[col])\n",
    "    \n",
    "    #print(df_train.head())\n",
    "    #print(df_test.head())\n",
    "    #print(df_train['lat'].isnull().sum())\n",
    "    \n",
    "    \"\"\"    \n",
    "    #convert categorical variable into integers\n",
    "    \n",
    "    #['violation_code', 'grafitti_status', 'city', \n",
    "    #'violation_description', 'agency_name', 'violation_street_name', 'disposition']\n",
    "    \n",
    "    LE = preprocessing.LabelEncoder()\n",
    "    \n",
    "    for obj in objs:\n",
    "        df_train[obj] = LE.fit_transform(df_train[obj])\n",
    "        df_test[obj] = LE.fit_transform(df_test[obj])\n",
    "    \"\"\"\n",
    "    #print('Train skew:', '\\n', df_train.skew(), '\\n')\n",
    "    #print('Train columns:', '\\n', df_train.columns, '\\n')\n",
    "    #print('Test skew:', '\\n', df_test.skew())\n",
    "    #print('Test columns:', '\\n', df_test.columns, '\\n')\n",
    "    \n",
    "    #highly skewed columns\n",
    "    skewed = ['discount_amount', 'violation_street_number']\n",
    "    df_train.drop(skewed, axis=1, inplace=True)\n",
    "    df_test.drop(skewed, axis=1, inplace=True)\n",
    "    #print(df_train.skew(),'\\t', df_test.skew())\n",
    "    #print(df_train.columns)\n",
    "    \n",
    "    train_cols = ['zip_code', 'fine_amount', 'late_fee', 'judgment_amount',\n",
    "       'mailing_address_str_name', 'disposition', 'mailing_address_str_number',\n",
    "       'agency_name', 'state', 'violation_description', 'clean_up_cost',\n",
    "       'city', 'violation_code', 'violation_street_name', 'lat',\n",
    "       'lon', 'days', 'compliance']\n",
    "    \n",
    "    df_train = df_train[train_cols]\n",
    "    #print(df_train.columns)\n",
    "    \n",
    "    \"\"\"\n",
    "    for col in df_train.columns:\n",
    "        df_train[col] = df_train[col].astype('float64')\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = df_test[col].astype('float64')\n",
    "    \n",
    "    \n",
    "    for col in df_train.columns[0:len(df_train.columns)-1]:\n",
    "        df_train[col] = stats.boxcox(df_train[col] + 1 - np.min(df_train[col]))[0]\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = stats.boxcox(df_test[col] + 1 - np.min(df_test[col]))[0]\n",
    "        \n",
    "    print('De-skewed train: ', df_train.skew())\n",
    "    print('De-skewed test: ', df_test.skew())\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #compute feature importance using ExtraTreeClassifier\n",
    "    \n",
    "    \"\"\"\n",
    "    forest = ExtraTreesClassifier(n_estimators=50)\n",
    "    feats_cols = df_train.columns[0:len(df_train.columns)-1]\n",
    "    X_selection = df_train[feats_cols]\n",
    "    y_selection = df_train['compliance']\n",
    "    forest.fit(X_selection, y_selection)\n",
    "    importances = forest.feature_importances_\n",
    "    new_cols = df_train.columns[np.argsort(importances)[::-1]]   #the order of most important features\n",
    "    #print(new_cols)\n",
    "    \n",
    "    for i in range(len(new_cols) - 1):\n",
    "        feats = new_cols[:i+1]\n",
    "        X = df_train[feats]\n",
    "        #print(X.columns)\n",
    "        #X = df.loc[:, df.columns != 'compliance']\n",
    "        #X.set_index('ticket_id', inplace=True)\n",
    "        y = df_train['compliance']\n",
    "        #print(X.head())\n",
    "        cols_test = df_test.columns\n",
    "        X_testF = df_test[cols_test]\n",
    "        #X_testF.set_index('ticket_id', inplace=True)\n",
    "        #print(X_testF.head())\n",
    "     \"\"\"   \n",
    "        #splitting X, y\n",
    "    \n",
    "    feats_cols = df_train.columns[0:len(df_train.columns)-1]\n",
    "    X = df_train[feats_cols]\n",
    "    y = df_train['compliance']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "        \n",
    "    \"\"\"\n",
    "        #normalizing\n",
    "        \n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaler = scaler.fit_transform(X_train)\n",
    "        X_test_scaler = scaler.fit_transform(X_test)\n",
    "        X_test_scalerF = scaler.fit_transform(X_testF)\n",
    "\n",
    "        #print(X_train_scaler.shape)\n",
    "        #print(X_test_scaler.shape)\n",
    "        #print(X_test_scalerF.shape)\n",
    "    \"\"\"\n",
    "        #set up the GradientBoostingClassifier\n",
    "        \n",
    "\n",
    "\n",
    "    clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        #clf = LogisticRegression(C=2.0, \n",
    "                                 #class_weight='balanced',\n",
    "                                 #random_state=0, \n",
    "                                 #solver='lbfgs', \n",
    "                                 #multi_class='ovr'\n",
    "                                #)\n",
    "\n",
    "        #clf = GaussianNB()\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        #GridSearchCV optimization\n",
    "        #from sklearn.grid_search import GridSearchCV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        for i in range(0,5):\n",
    "            randomly_sampled = df.ix[np.random.choice(df.index, 1000)]\n",
    "\n",
    "\n",
    "        gb_grid_params = {'max_depth': [4, 6, 8, 10, 12, 14, 16],\n",
    "                          'min_samples_split': [200, 400, 500, 700, 1000],\n",
    "                  }\n",
    "\n",
    "        gbc_grid = GridSearchCV(gbc,\n",
    "                                gb_grid_params,\n",
    "                                cv=5,\n",
    "                                scoring='roc_auc', \n",
    "                                n_jobs=-1)\n",
    "\n",
    "\n",
    "        #gbc_grid.fit(X_train_scaler, y_train)\n",
    "        #print('Grid best AUC', gbc_grid.best_params_)\n",
    "        #print('Best AUC is', gbc_grid.best_score)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    y_score_clf = clf.fit(X_train, y_train).predict(X_test)\n",
    "    fpr_clf, tpr_clf, _ = roc_curve(y_test, y_score_clf)\n",
    "    roc_auc_clf = auc(fpr_clf, tpr_clf)\n",
    "    print('AUC score is:', roc_auc_clf)\n",
    "        \n",
    "    #toreturn = clf.predict_proba(X_test_scalerF)\n",
    "    #print(toreturn[:10,1])\n",
    "    #answer = pd.Series(data=toreturn[:,1], index=df_test.index, dtype='float32', name=None, copy=False, fastpath=False)\n",
    "    #answer.index = answer.index.astype('int32')\n",
    "    #print(answer.head())\n",
    "    #print(df_test.index)\n",
    "    #print(toreturn)\n",
    "    \n",
    "    \n",
    "    return (clf.score(X_train_, y_train), clf.score(X_test, y_test))\n",
    "    #return (gbc.score(X_train_scaler, y_train), gbc.score(X_test_scaler, y))\n",
    "    #return 'none'\n",
    "    #return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "def blight_model():\n",
    "    #pd.set_option('display.max_columns', None)\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    #reading train dataset\n",
    "    df_train = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_test = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_addresses = pd.read_csv(\"addresses.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_latlons = pd.read_csv(\"latlons.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    \n",
    "    #getting the common features into a list\n",
    "    commons_test = list(set(df_train.columns).intersection(df_test.columns))   #test features\n",
    "    commons_train = commons_test + ['compliance']    #train features and labels\n",
    "\n",
    "    df_train = df_train[commons_train]\n",
    "    df_test = df_test[commons_test]\n",
    "        \n",
    "    #join 'addresses' and 'latlons' on 'address' column\n",
    "    df_joined = df_addresses.set_index('address').join(df_latlons.set_index('address'),\n",
    "                                                       how='outer')\n",
    "    \n",
    "    #join 'joined' on 'train' and 'test', on 'ticket_id' as index\n",
    "    df_train = df_train.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    df_test = df_test.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    \n",
    "    #cleaning datasets\n",
    "    \n",
    "    #print(pd.isnull(df_train['compliance']).sum())\n",
    "    #df_train['compliance'].fillna(\"NaN\", inplace=True)\n",
    "    df_train.dropna(subset=['compliance'], inplace=True)\n",
    "    df_train['compliance'] = df_train['compliance'].astype('int32')\n",
    "    #print(df_train['compliance'].isnull().sum())\n",
    "    #print(df_train['compliance'].head(20))\n",
    "    \n",
    "    #print(df_train.shape, '\\t', df_test.shape)\n",
    "    \n",
    "    #check for full nulls column\n",
    "    #for col in df_test.columns:\n",
    "    #    print(col,'\\t', df_train[col].isnull().sum())\n",
    "    #    print(col,'\\t', df_test[col].isnull().sum())\n",
    "    \n",
    "    #drop columns\n",
    "    to_drop = ['admin_fee', 'state_fee', 'violation_zip_code',\n",
    "              'grafitti_status', 'non_us_str_code', 'inspector_name',\n",
    "              'violator_name', 'country', 'discount_amount', 'violation_street_number',\n",
    "              'mailing_address_str_number', \n",
    "              #'clean_up_cost',\n",
    "              #'agency_name'\n",
    "              ]\n",
    "    \n",
    "    df_train.drop(to_drop, axis=1, inplace=True)\n",
    "    df_test.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    #drop NaN values in train dataset and replace NaN with mean in test dataset\n",
    "    \n",
    "    df_train.dropna(inplace=True)\n",
    "\n",
    "    df_test.fillna(value=0, inplace=True)\n",
    "    \n",
    "    #print(df_test.isnull().sum())\n",
    "    \n",
    "    #for col in df_test.columns:\n",
    "    #    print(col,'\\t', df_train[col].isnull().sum())\n",
    "    #    print(col,'\\t', df_test[col].isnull().sum())\n",
    "        \n",
    "    # so far no zeros in train and test df\n",
    "    \n",
    "    #transform date columns delta into float, 'hearing_date' and 'payment_date'\n",
    "\n",
    "    df_train['hearing_date'] = pd.to_datetime(df_train['hearing_date'])\n",
    "    df_train['ticket_issued_date'] = pd.to_datetime(df_train['ticket_issued_date'])\n",
    "    df_train['days'] = df_train['hearing_date'] - df_train['ticket_issued_date']\n",
    "    df_train.days = (df_train.days.dt.days).astype('float64')\n",
    "    df_train.drop(['ticket_issued_date', 'hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    #for test data too\n",
    "    \n",
    "    df_test['hearing_date'] = pd.to_datetime(df_test['hearing_date'])\n",
    "    df_test['ticket_issued_date'] = pd.to_datetime(df_test['ticket_issued_date'])\n",
    "    df_test['days'] = df_test['hearing_date'] - df_test['ticket_issued_date']\n",
    "    df_test.days = (df_test.days.dt.days).astype('float64')\n",
    "    df_test.drop(['ticket_issued_date','hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    df_test['city'] = df_test['city'].astype('str')   #cast any non-str to str in 'city' column\n",
    "    \n",
    "    #print('Train variation is: ', df_train.var(),'\\n')\n",
    "    #rint('Train correlation is: ', df_train.corr(method='pearson'),'\\n')\n",
    "    \n",
    "    #print('Test variation is: ', df_test.var(),'\\n')\n",
    "    #print('Test correlation is: ', df_test.corr(method='pearson'),'\\n')\n",
    "    \n",
    "    #print('Train clean_up_cost is: ', df_train['clean_up_cost'].isnull().sum())\n",
    "    \n",
    "    \n",
    "    #getting non numerical columns\n",
    "    objs = []\n",
    "    for col in df_test.columns:\n",
    "        if df_test[col].dtype == 'object':\n",
    "            objs.append(col)\n",
    "    \"\"\"\n",
    "    for obj in objs:\n",
    "        mean = df_train.groupby(obj).compliance.mean()\n",
    "        def apm(val):\n",
    "            return mean.loc[val]\n",
    "        df_train[obj] = apm(df_train[obj].values).values\n",
    "        df_test[obj] = apm(df_test[obj].values).values\n",
    "    \"\"\"\n",
    "    \n",
    "    #reorganizing train cols\n",
    "    train_cols = ['city', \n",
    "                  'clean_up_cost', \n",
    "                  'violation_description',\n",
    "                  'judgment_amount',\n",
    "                  'mailing_address_str_name',\n",
    "                  'zip_code', \n",
    "                  'fine_amount', \n",
    "                  'agency_name',\n",
    "                  'disposition',\n",
    "                  'violation_code', \n",
    "                  'late_fee', \n",
    "                  'state',\n",
    "                  'violation_street_name', \n",
    "                  'lat',\n",
    "                  'lon', \n",
    "                  'days', \n",
    "                  'compliance']\n",
    "    df_train = df_train[train_cols]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    imp = Imputer(missing_values='NaN', strategy=\"mean\", axis=0)\n",
    "    \n",
    "    \n",
    "    for col in df_train[0:len(df_train.columns)-1]:\n",
    "        df_train[col] = imp.fit_transform(df_train[col])\n",
    "        \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = imp.fit_transform(df_test[col])\n",
    "    \n",
    "    df_train_imp = pd.DataFrame(imp.fit_transform(df_train))\n",
    "    df_train_imp.columns = df_train.columns\n",
    "    df_train_imp.index = df_train.index\n",
    "    \n",
    "    df_test_imp = pd.DataFrame(imp.fit_transform(df_test))\n",
    "    df_test_imp.columns = df_test.columns\n",
    "    df_test_imp.index = df_test.index\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    #convert categorical variable into integers\n",
    "    \n",
    "    #['violation_code', 'grafitti_status', 'city', \n",
    "    #'violation_description', 'agency_name', 'violation_street_name', 'disposition']\n",
    "    \n",
    "    LE = preprocessing.LabelEncoder()\n",
    "    \n",
    "    for obj in objs:\n",
    "        df_train[obj] = LE.fit_transform(df_train[obj])\n",
    "        df_test[obj] = LE.fit_transform(df_test[obj])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print('Train skew:', '\\n', df_train.skew(), '\\n')\n",
    "    #print('Train columns:', '\\n', df_train.columns, '\\n')\n",
    "    #print('Test skew:', '\\n', df_test.skew())\n",
    "    #print('Test columns:', '\\n', df_test.columns, '\\n')\n",
    "    \n",
    "    \"\"\"\n",
    "    for col in df_train.columns:\n",
    "        df_train[col] = df_train[col].astype('float64')\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = df_test[col].astype('float64')\n",
    "    \n",
    "    \n",
    "    for col in df_train.columns[0:len(df_train.columns)-1]:\n",
    "        df_train[col] = np.log(df_train[col] + 1 - np.min(df_train[col]))\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = np.log(df_test[col] + 1 - np.min(df_test[col]))\n",
    "    \n",
    "    df_test.fillna(value=0, inplace=True)\n",
    "    \"\"\"\n",
    "    #print(df_test.isnull().sum())\n",
    "    \n",
    "    #print('De-skewed train: ', df_train.skew())\n",
    "    #print('De-skewed test: ', df_test.skew())\n",
    "    \n",
    "    df_train.replace([np.inf, -np.inf], np.nan).dropna(inplace=True)   #droping infinite values\n",
    "    #print(df_train.shape)\n",
    "    \n",
    "    #compute feature importance using ExtraTreeClassifier\n",
    "    \n",
    "    \"\"\"\n",
    "    forest = ExtraTreesClassifier(n_estimators=50)\n",
    "    feats_cols = df_train.columns[0:len(df_train.columns)-1]\n",
    "    X_selection = df_train[feats_cols]\n",
    "    y_selection = df_train['compliance']\n",
    "    forest.fit(X_selection, y_selection)\n",
    "    importances = forest.feature_importances_\n",
    "    new_cols = df_train.columns[np.argsort(importances)[::-1]]   #the order of most important features\n",
    "    #print(new_cols)\n",
    "    \n",
    "    for i in range(len(new_cols) - 1):\n",
    "        feats = new_cols[:i+1]\n",
    "        X = df_train[feats]\n",
    "        #print(X.columns)\n",
    "        #X = df.loc[:, df.columns != 'compliance']\n",
    "        #X.set_index('ticket_id', inplace=True)\n",
    "        y = df_train['compliance']\n",
    "        #print(X.head())\n",
    "        cols_test = df_test.columns\n",
    "        X_testF = df_test[cols_test]\n",
    "        #X_testF.set_index('ticket_id', inplace=True)\n",
    "        #print(X_testF.head())\n",
    "     \"\"\"\n",
    "    \n",
    "    #splitting X, y\n",
    "    feats_cols = df_train.columns[0:len(df_train.columns)-1]\n",
    "    #print(df_train.columns)\n",
    "    #print(feats_cols)\n",
    "    #print(df_train['compliance'].head(15))\n",
    "    X = df_train[feats_cols]\n",
    "    y = df_train['compliance']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    #test set X\n",
    "    X_testF = df_test[df_test.columns]\n",
    "    \n",
    "    #print(X_testF.isnull().sum())\n",
    "    \n",
    "    #normalizing\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaler = scaler.fit_transform(X_train)\n",
    "    X_test_scaler = scaler.fit_transform(X_test)\n",
    "    X_test_scalerF = scaler.fit_transform(X_testF)\n",
    "\n",
    "        #print(X_train_scaler.shape)\n",
    "        #print(X_test_scaler.shape)\n",
    "        #print(X_test_scalerF.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #set up the GradientBoostingClassifier\n",
    "    #clf = GradientBoostingClassifier(n_estimators=8, learning_rate=1.4, \n",
    "    #                                 max_depth=1, random_state=2)\n",
    "\n",
    "    \n",
    "    clf = LogisticRegression(C=1.3, \n",
    "                            class_weight='balanced',\n",
    "                            random_state=2, \n",
    "                            solver='lbfgs', \n",
    "                            multi_class='ovr'\n",
    "                            )\n",
    "    \n",
    "    #clf = GaussianNB()   \n",
    "\n",
    "    \"\"\"\n",
    "        #GridSearchCV optimization\n",
    "        #from sklearn.grid_search import GridSearchCV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        for i in range(0,5):\n",
    "            randomly_sampled = df.ix[np.random.choice(df.index, 1000)]\n",
    "\n",
    "\n",
    "        gb_grid_params = {'max_depth': [4, 6, 8, 10, 12, 14, 16],\n",
    "                          'min_samples_split': [200, 400, 500, 700, 1000],\n",
    "                  }\n",
    "\n",
    "        gbc_grid = GridSearchCV(gbc,\n",
    "                                gb_grid_params,\n",
    "                                cv=5,\n",
    "                                scoring='roc_auc', \n",
    "                                n_jobs=-1)\n",
    "\n",
    "\n",
    "        #gbc_grid.fit(X_train_scaler, y_train)\n",
    "        #print('Grid best AUC', gbc_grid.best_params_)\n",
    "        #print('Best AUC is', gbc_grid.best_score)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    y_score_clf = clf.fit(X_train_scaler, y_train).predict(X_test_scaler)\n",
    "    fpr_clf, tpr_clf, _ = roc_curve(y_test, y_score_clf)\n",
    "    roc_auc_clf = auc(fpr_clf, tpr_clf)\n",
    "    #print('AUC score is:', roc_auc_clf)\n",
    "        \n",
    "    toreturn = clf.predict_proba(X_test_scalerF)\n",
    "    #print(toreturn[:10,1])\n",
    "    answer = pd.Series(data=toreturn[:,1], index=df_test.index, dtype='float32', name=None, copy=False, fastpath=False)\n",
    "    #answer.index = answer.index.astype('int32')\n",
    "    #print(answer.head())\n",
    "    #print(df_test.index)\n",
    "    #print(toreturn)\n",
    "    \n",
    "    \n",
    "    #return (clf.score(X_train_scaler, y_train), clf.score(X_test_scaler, y_test))\n",
    "    #return 'none'\n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
