{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def blight_model():\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    #reading train dataset\n",
    "    df_train = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_test = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_addresses = pd.read_csv(\"addresses.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    df_latlons = pd.read_csv(\"latlons.csv\", encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    \n",
    "    #getting the common features into a list\n",
    "    commons_test = list(set(df_train.columns).intersection(df_test.columns))\n",
    "    commons_train = commons_test + ['compliance']\n",
    "\n",
    "    df_train = df_train[commons_train]\n",
    "    df_test = df_test[commons_test]\n",
    "        \n",
    "    #join 'addresses' and 'latlons' on 'address' column\n",
    "    df_joined = df_addresses.set_index('address').join(df_latlons.set_index('address'),\n",
    "                                                       how='outer')\n",
    "    \n",
    "    #join 'joined' on 'train' and 'test', on 'ticket_id' as index\n",
    "    df_train = df_train.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    df_test = df_test.set_index('ticket_id').join(df_joined.set_index('ticket_id'),\n",
    "                                                   how='inner')\n",
    "    \n",
    "    #cleaning datasets  \n",
    "    df_train.dropna(subset=['compliance'], how='any', inplace=True)\n",
    "    \n",
    "    df_train.dropna(subset=['compliance'], inplace=True)\n",
    "    df_train['compliance'] = df_train['compliance'].astype('int32')\n",
    "    \n",
    "    df_train[['compliance']] = df_train[['compliance']].astype('float64')\n",
    "\n",
    "    #check for full nulls column\n",
    "    #for col in df_test.columns:\n",
    "    #    print(col,'\\t', df_train[col].isnull().sum())\n",
    "    #    print(col,'\\t', df_test[col].isnull().sum())\n",
    "    \n",
    "    #transform date columns delta into float, 'hearing_date' and 'payment_date'\n",
    "\n",
    "    df_train['hearing_date'] = pd.to_datetime(df_train['hearing_date'])\n",
    "    df_train['ticket_issued_date'] = pd.to_datetime(df_train['ticket_issued_date'])\n",
    "    df_train['days'] = df_train['hearing_date'] - df_train['ticket_issued_date']\n",
    "    df_train.days = (df_train.days.dt.days).astype('float64')\n",
    "    df_train.drop(['ticket_issued_date', 'hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    #for test data too\n",
    "    df_test['hearing_date'] = pd.to_datetime(df_test['hearing_date'])\n",
    "    df_test['ticket_issued_date'] = pd.to_datetime(df_test['ticket_issued_date'])\n",
    "    df_test['days'] = df_test['hearing_date'] - df_test['ticket_issued_date']\n",
    "    df_test.days = (df_test.days.dt.days).astype('float64')\n",
    "    df_test.drop(['ticket_issued_date','hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "    to_drop = ['admin_fee', 'state_fee', 'violation_zip_code',\n",
    "              'grafitti_status', 'non_us_str_code', 'inspector_name',\n",
    "              'violator_name', 'country', 'discount_amount', 'violation_street_number',\n",
    "              'mailing_address_str_number']\n",
    "    \n",
    "    df_train.drop(to_drop, axis=1, inplace=True)\n",
    "    df_test.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    #drop NaN values in train dataset and replace NaNs in test dataset\n",
    "    df_train.dropna(inplace=True)\n",
    "    df_test.fillna(value=0, inplace=True)\n",
    "    \n",
    "    #getting non numerical columns\n",
    "    objs = []\n",
    "    for col in df_test.columns:\n",
    "        if df_test[col].dtype == 'object':\n",
    "            objs.append(col)\n",
    "            \n",
    "    #for col in df_test.columns:\n",
    "    #    print(col,'\\t', df_train[col].isnull().sum())\n",
    "    #    print(col,'\\t', df_test[col].isnull().sum())\n",
    "    \n",
    "    for col in df_train.columns:\n",
    "        print(df_train[col].dtype)\n",
    "    \n",
    "    print(df_train.head())\n",
    "    \n",
    "    #convert categorical variable into integers\n",
    "    \n",
    "    \"\"\"\n",
    "    LE = preprocessing.LabelEncoder()\n",
    "    \n",
    "    for obj in objs:\n",
    "        df_train[obj] = LE.fit_transform(df_train[obj])\n",
    "        df_test[obj] = LE.fit_transform(df_test[obj])\n",
    "    \n",
    "    #print('Train skew:', '\\n', df_train.skew(), '\\n')\n",
    "    #print('Train columns:', '\\n', df_train.columns, '\\n')\n",
    "    #print('Test skew:', '\\n', df_test.skew())\n",
    "    #print('Test columns:', '\\n', df_test.columns, '\\n')\n",
    "    \n",
    "    \n",
    "    for col in df_train.columns:\n",
    "        df_train[col] = df_train[col].astype('float64')\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = df_test[col].astype('float64')\n",
    "    \"\"\"    \n",
    "    #reorganizing train cols\n",
    "    train_cols = ['city', \n",
    "                  'clean_up_cost', \n",
    "                  'violation_description',\n",
    "                  'judgment_amount',\n",
    "                  'mailing_address_str_name',\n",
    "                  'zip_code', \n",
    "                  'fine_amount', \n",
    "                  'agency_name',\n",
    "                  'disposition',\n",
    "                  'violation_code', \n",
    "                  'late_fee', \n",
    "                  'state',\n",
    "                  'violation_street_name', \n",
    "                  'lat',\n",
    "                  'lon', \n",
    "                  'days', \n",
    "                  'compliance']\n",
    "    \n",
    "    df_train = df_train[train_cols]\n",
    "    #print(df_train.columns)\n",
    "    \n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    for col in df_train.columns[0:len(df_train.columns)-1]:\n",
    "        df_train[col] = stats.boxcox(df_train[col] + 1 - np.min(df_train[col]))[0]\n",
    "    \n",
    "    for col in df_test.columns:\n",
    "        df_test[col] = stats.boxcox(df_test[col] + 1 - np.min(df_test[col]))[0]\n",
    "        \n",
    "    #print('De-skewed train: ', df_train.skew())\n",
    "    #print('De-skewed test: ', df_test.skew())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #compute feature importance using ExtraTreeClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    \n",
    "    forest = ExtraTreesClassifier(n_estimators=50)\n",
    "    feats_cols = df_train.columns[0:len(df_train.columns)-1]\n",
    "    X_selection = df_train[feats_cols]\n",
    "    y_selection = df_train['compliance']\n",
    "    forest.fit(X_selection, y_selection)\n",
    "    importances = forest.feature_importances_\n",
    "    new_cols = df_train.columns[np.argsort(importances)[::-1]]   #the order of most important features\n",
    "    #print(new_cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(new_cols) - 1):\n",
    "        feats = new_cols[:i+1]\n",
    "        X = df_train[feats]\n",
    "        #print(X.columns)\n",
    "        #X = df.loc[:, df.columns != 'compliance']\n",
    "        #X.set_index('ticket_id', inplace=True)\n",
    "        y = df_train['compliance']\n",
    "        #print(X.head())\n",
    "        cols_test = df_test.columns\n",
    "        X_testF = df_test[cols_test]\n",
    "        #X_testF.set_index('ticket_id', inplace=True)\n",
    "        #print(X_testF.head())\n",
    "        \n",
    "        #splitting X, y\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "        #normalizing\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaler = scaler.fit_transform(X_train)\n",
    "        X_test_scaler = scaler.fit_transform(X_test)\n",
    "        X_test_scalerF = scaler.fit_transform(X_testF)\n",
    "\n",
    "        #print(X_train_scaler.shape)\n",
    "        #print(X_test_scaler.shape)\n",
    "        #print(X_test_scalerF.shape)\n",
    "\n",
    "        #set up the GradientBoostingClassifier\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "            max_depth=6, random_state=0).fit(X_train_scaler, y_train)\n",
    "\n",
    "\n",
    "        #clf = LogisticRegression(C=2.0, \n",
    "                                 #class_weight='balanced',\n",
    "                                 #random_state=0, \n",
    "                                 #solver='lbfgs', \n",
    "                                 #multi_class='ovr'\n",
    "                                #)\n",
    "\n",
    "        #clf = GaussianNB()\n",
    "    \n",
    "\n",
    "        \"\"\"\n",
    "        #GridSearchCV optimization\n",
    "        #from sklearn.grid_search import GridSearchCV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        for i in range(0,5):\n",
    "            randomly_sampled = df.ix[np.random.choice(df.index, 1000)]\n",
    "\n",
    "\n",
    "        gb_grid_params = {'max_depth': [4, 6, 8, 10, 12, 14, 16],\n",
    "                          'min_samples_split': [200, 400, 500, 700, 1000],\n",
    "                  }\n",
    "\n",
    "        gbc_grid = GridSearchCV(gbc,\n",
    "                                gb_grid_params,\n",
    "                                cv=5,\n",
    "                                scoring='roc_auc', \n",
    "                                n_jobs=-1)\n",
    "\n",
    "\n",
    "        #gbc_grid.fit(X_train_scaler, y_train)\n",
    "        #print('Grid best AUC', gbc_grid.best_params_)\n",
    "        #print('Best AUC is', gbc_grid.best_score)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        y_score_clf = clf.fit(X_train_scaler, y_train).predict(X_test_scaler)\n",
    "        fpr_clf, tpr_clf, _ = roc_curve(y_test, y_score_clf)\n",
    "        roc_auc_clf = auc(fpr_clf, tpr_clf)\n",
    "        print('AUC score is:', roc_auc_clf)\n",
    "        \n",
    "    #toreturn = clf.predict_proba(X_test_scalerF)\n",
    "    #print(toreturn[:10,1])\n",
    "    #answer = pd.Series(data=toreturn[:,1], index=df_test.index, dtype='float32', name=None, copy=False, fastpath=False)\n",
    "    #answer.index = answer.index.astype('int32')\n",
    "    #print(answer.head())\n",
    "    #print(df_test.index)\n",
    "    #print(toreturn)\n",
    "    \n",
    "    \n",
    "    return (clf.score(X_train_scaler, y_train), clf.score(X_test_scaler, y_test))\n",
    "    #return (gbc.score(X_train_scaler, y_train), gbc.score(X_test_scaler, y))\n",
    "    #return 'none'\n",
    "    #return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "object\n",
      "object\n",
      "object\n",
      "object\n",
      "float64\n",
      "object\n",
      "object\n",
      "float64\n",
      "object\n",
      "object\n",
      "float64\n",
      "object\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "           fine_amount state violation_code violation_street_name  \\\n",
      "ticket_id                                                           \n",
      "22056            250.0    IL      9-1-36(a)                 TYLER   \n",
      "27586            750.0    MI     61-63.0600               CENTRAL   \n",
      "22046            250.0    CA      9-1-36(a)            NORTHFIELD   \n",
      "18738            750.0    MI     61-63.0500             BRENTWOOD   \n",
      "18735            100.0    MI     61-63.0100            MT ELLIOTT   \n",
      "\n",
      "                                              agency_name  judgment_amount  \\\n",
      "ticket_id                                                                    \n",
      "22056      Buildings, Safety Engineering & Env Department            305.0   \n",
      "27586      Buildings, Safety Engineering & Env Department            855.0   \n",
      "22046      Buildings, Safety Engineering & Env Department            305.0   \n",
      "18738      Buildings, Safety Engineering & Env Department            855.0   \n",
      "18735      Buildings, Safety Engineering & Env Department            140.0   \n",
      "\n",
      "            zip_code       city  late_fee mailing_address_str_name  \\\n",
      "ticket_id                                                            \n",
      "22056          60606    CHICAGO      25.0                S. WICKER   \n",
      "27586          48208    Detroit      75.0       Martin Luther King   \n",
      "22046      908041512  LOG BEACH      25.0                  E. 17TH   \n",
      "18738          48038    Clinton      75.0                 Garfield   \n",
      "18735          48211    Detroit      10.0              Mt. Elliott   \n",
      "\n",
      "                                       violation_description  clean_up_cost  \\\n",
      "ticket_id                                                                     \n",
      "22056      Failure of owner to obtain certificate of comp...            0.0   \n",
      "27586      Failed To Secure Permit For Lawful Use Of Buil...            0.0   \n",
      "22046      Failure of owner to obtain certificate of comp...            0.0   \n",
      "18738         Failed To Secure Permit For Lawful Use Of Land            0.0   \n",
      "18735                  Noncompliance/Grant Condition/BZA/BSE            0.0   \n",
      "\n",
      "                            disposition  compliance        lat        lon  \\\n",
      "ticket_id                                                                   \n",
      "22056            Responsible by Default         0.0  42.390729 -83.124268   \n",
      "27586      Responsible by Determination         1.0  42.326937 -83.135118   \n",
      "22046            Responsible by Default         0.0  42.145257 -83.208233   \n",
      "18738            Responsible by Default         0.0  42.433466 -83.023493   \n",
      "18735            Responsible by Default         0.0  42.388641 -83.037858   \n",
      "\n",
      "            days  \n",
      "ticket_id         \n",
      "22056      369.0  \n",
      "27586      378.0  \n",
      "22046      323.0  \n",
      "18738      253.0  \n",
      "18735      251.0  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-baf5c1d6201e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblight_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e29c2f61b1d2>\u001b[0m in \u001b[0;36mblight_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_cols' is not defined"
     ]
    }
   ],
   "source": [
    "blight_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
